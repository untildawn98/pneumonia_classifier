{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nYqLbc5YybqP",
    "outputId": "ee892ee0-3bf1-48dd-ef3d-4b392e73140f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# !pip install tensorflow\n",
    "# !pip install keras\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install numpy\n",
    "# !pip install seaborn\n",
    "# !pip install sklearn\n",
    "# !pip install glob\n",
    "# !pip install cv2\n",
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jD6aBrrRyaLb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd       \n",
    "import matplotlib as mat\n",
    "import matplotlib.pyplot as plt  \n",
    "import matplotlib.image as mpimg   \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.metrics import AUC, Accuracy, Precision, SensitivityAtSpecificity\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import glob\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95qiNVy5yaLd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19 \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import time\n",
    "initial_time = time.time() #Time for the notebook starting to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xwse6Z4yaLd"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH = 32\n",
    "SEED = 42\n",
    "IMG_SIZE_EFF=229\n",
    "epos = 30 # Number of epochs to train\n",
    "oversampling_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHMJYR3xyaLe"
   },
   "outputs": [],
   "source": [
    "main_path = \"C:/Users/nourn/Desktop/post_grad/winter/machine learning/project/pnemonia/code/data/chest_xray/chest_xray/\"\n",
    "train_path = os.path.join(main_path,\"train\")\n",
    "test_path=os.path.join(main_path,\"test\")\n",
    "\n",
    "train_normal = glob.glob(train_path+\"/NORMAL/*.jpeg\")\n",
    "train_pneumonia = glob.glob(train_path+\"/PNEUMONIA/*.jpeg\")\n",
    "\n",
    "test_normal = glob.glob(test_path+\"/NORMAL/*.jpeg\")\n",
    "test_pneumonia = glob.glob(test_path+\"/PNEUMONIA/*.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOvHQ7p2yaLe"
   },
   "outputs": [],
   "source": [
    "train_list = [x for x in train_normal]\n",
    "train_list.extend([x for x in train_pneumonia])\n",
    "\n",
    "df_train = pd.DataFrame(np.concatenate([['Normal']*len(train_normal) , ['Pneumonia']*len(train_pneumonia)]), columns = ['class'])\n",
    "df_train['image'] = [x for x in train_list]\n",
    "\n",
    "test_list = [x for x in test_normal]\n",
    "test_list.extend([x for x in test_pneumonia])\n",
    "\n",
    "df_test = pd.DataFrame(np.concatenate([['Normal']*len(test_normal) , ['Pneumonia']*len(test_pneumonia)]), columns = ['class'])\n",
    "df_test['image'] = [x for x in test_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2ywYwvUyaLe"
   },
   "source": [
    "# <a id=\"3\">Exploring the Data</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go4v4I44yaLg"
   },
   "source": [
    "Let's check the target distribution on each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "0TcdKc6YyaLg",
    "outputId": "87b74f23-dc35-4e46-ee40-35ae34dccd03"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "ax = sns.countplot(x='class', data=df_train, palette=\"mako\")\n",
    "\n",
    "plt.xlabel(\"Class\", fontsize= 12)\n",
    "plt.ylabel(\"# of Samples\", fontsize= 12)\n",
    "plt.ylim(0,5000)\n",
    "plt.xticks([0,1], ['Normal', 'Pneumonia'], fontsize = 11)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate((p.get_height()), (p.get_x()+0.30, p.get_height()+300), fontsize = 13)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "ZHN5Z48hyaLi",
    "outputId": "48bda210-6ce9-41bd-d89b-1455249d16fc"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "df_train['class'].value_counts().plot(kind='pie',labels = ['',''], autopct='%1.1f%%', colors = ['darkcyan','blue'], explode = [0,0.05], textprops = {\"fontsize\":15})\n",
    "\n",
    "plt.legend(labels=['Pneumonia', 'Normal'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "id": "VgeTG8YpyaLj",
    "outputId": "a58d204d-1a4f-4ef4-e4dd-8fbaf9d1e29f"
   },
   "outputs": [],
   "source": [
    "print('Train Set - Normal')\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "for i in range(0, 12):\n",
    "    plt.subplot(3,4,i + 1)\n",
    "    img = mpimg.imread(train_normal[i])\n",
    "    plt.imshow(img,cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "id": "vfNbTThWyaLk",
    "outputId": "b3f8a395-e3e5-42ef-a8af-6242b20546f6"
   },
   "outputs": [],
   "source": [
    "print('Train Set - Pneumonia')\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "for i in range(0, 12):\n",
    "    plt.subplot(3,4,i + 1)\n",
    "    img = mpimg.imread(train_pneumonia[i])\n",
    "    plt.imshow(img,cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s5IM4-RyaLl"
   },
   "source": [
    "# <a id=\"4\">Preparing the Data</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Asfn8wfLyaLm"
   },
   "source": [
    "First, we need to create a validation set. To do that, we apply a simple stratified split on the original train dataset, using 80% for actual training and 20% for validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kkcce3ayaLm"
   },
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df_train, test_size = 0.20, random_state = SEED, stratify = df_train['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if oversampling_flag:\n",
    "    df_normal = train_df[train_df['class'] =='Normal']\n",
    "    df_pneumonia =  train_df[train_df['class'] =='Pneumonia']\n",
    "\n",
    "    size_diff = len(df_normal) - len(df_pneumonia)\n",
    "    # If the pneumonia class has fewer samples than normal class\n",
    "    if size_diff > 0:\n",
    "        # Upsample the pneumonia class to match the size of the normal class\n",
    "        df_pneumonia_upsampled = resample(df_pneumonia, replace=True, n_samples=size_diff+len(df_pneumonia))\n",
    "        # Concatenate the upsampled pneumonia class with the normal class\n",
    "        train_df = pd.concat([df_normal, df_pneumonia_upsampled], ignore_index=True)\n",
    "    else:\n",
    "        # Upsample the normal class to match the size of the pneumonia class\n",
    "        df_normal_upsampled = resample(df_normal, replace=True, n_samples=-size_diff+len(df_normal))\n",
    "        # Concatenate the upsampled normal class with the pneumonia class\n",
    "        train_df = pd.concat([df_pneumonia, df_normal_upsampled], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEeqjsJ_yaLm"
   },
   "source": [
    "## dataloading\n",
    "Now, weâ€™re going to load the images from the folders and prepare them to feed our models. \n",
    "\n",
    "We begin by defining the data generators. With Keras Image Data Generator, we can rescale the pixel values and apply random transformation techniques for data augmentation on the fly. We define two different generators. The val_datagen is used to simply rescale the validation and test sets. The train_datagen includes some transformations to augment the train set.\n",
    "\n",
    "We apply those generators on each dataset using the flow_from_dataframe method. Apart from the transformations defined in each generator, the images are also resized based on the target_size set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ItOStyuayaLt",
    "outputId": "bbdbe907-0790-4f5b-ee23-6f5caa017a13"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1/255.,\n",
    "                                  zoom_range = 0.1,\n",
    "                                  #rotation_range = 0.1,\n",
    "                                  width_shift_range = 0.1,\n",
    "                                  height_shift_range = 0.1)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "ds_train = train_datagen.flow_from_dataframe(train_df,\n",
    "                                             x_col = 'image',\n",
    "                                             y_col = 'class',\n",
    "                                             target_size = (IMG_SIZE, IMG_SIZE),\n",
    "                                             class_mode = 'binary',\n",
    "                                             batch_size = BATCH,\n",
    "                                             seed = SEED)\n",
    "\n",
    "ds_val = val_datagen.flow_from_dataframe(val_df,\n",
    "                                            x_col = 'image',\n",
    "                                            y_col = 'class',\n",
    "                                            target_size = (IMG_SIZE, IMG_SIZE),\n",
    "                                            class_mode = 'binary',\n",
    "                                            batch_size = BATCH,\n",
    "                                            seed = SEED)\n",
    "\n",
    "ds_test = val_datagen.flow_from_dataframe(df_test,\n",
    "                                            x_col = 'image',\n",
    "                                            y_col = 'class',\n",
    "                                            target_size = (IMG_SIZE, IMG_SIZE),\n",
    "                                            class_mode = 'binary',\n",
    "                                            batch_size = 1,\n",
    "                                            shuffle = False)\n",
    "\n",
    "\n",
    "ds_train_efficient = train_datagen.flow_from_dataframe(train_df,\n",
    "                                             x_col = 'image',\n",
    "                                             y_col = 'class',\n",
    "                                             target_size = (IMG_SIZE_EFF, IMG_SIZE_EFF),\n",
    "                                             class_mode = 'binary',\n",
    "                                             batch_size = BATCH,\n",
    "                                             seed = SEED)\n",
    "\n",
    "ds_val_efficient = val_datagen.flow_from_dataframe(val_df,\n",
    "                                            x_col = 'image',\n",
    "                                            y_col = 'class',\n",
    "                                            target_size = (IMG_SIZE_EFF, IMG_SIZE_EFF),\n",
    "                                            class_mode = 'binary',\n",
    "                                            batch_size = BATCH,\n",
    "                                            seed = SEED)\n",
    "\n",
    "ds_test_efficient = val_datagen.flow_from_dataframe(df_test,\n",
    "                                            x_col = 'image',\n",
    "                                            y_col = 'class',\n",
    "                                            target_size = (IMG_SIZE_EFF, IMG_SIZE_EFF),\n",
    "                                            class_mode = 'binary',\n",
    "                                            batch_size = 1,\n",
    "                                            shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "kzz3TdYEyaLu",
    "outputId": "a9f8a17c-bb33-4039-ff74-41eaaeea164f"
   },
   "outputs": [],
   "source": [
    "# retrieve a batch of images from the data generator\n",
    "x_batch, y_batch = next(ds_train)\n",
    "# plot the first 9 images in the batch\n",
    "for i in range(32):\n",
    "    # define subplot\n",
    "    plt.subplot(4,8,i + 1)\n",
    "    # plot raw pixel data\n",
    "    plt.imshow(x_batch[i])\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_arr = class_weight.compute_class_weight('balanced',\n",
    "                                                 classes=np.unique(ds_train.classes), y=ds_train.classes)\n",
    "\n",
    "\n",
    "if oversampling_flag:   #use the previously calculated values\n",
    "    class_weights={0: 1.9440820130475303, 1: 0.6731203614069055}  \n",
    "else:\n",
    "    class_weights_arr[1]\n",
    "    class_weights={0: class_weights_arr[0], 1: class_weights_arr[1]}                                        \n",
    "    print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOkK_oPuyaLu"
   },
   "source": [
    "Now, we are ready for the next stage: creating and training the image classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSb0nGegtPF2"
   },
   "source": [
    "# <a id=\"5\">Transfer Learning</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7t9OlfQ5yaOF"
   },
   "source": [
    "The approach, called transfer learning, consists of using a pretrained model as a feature extractor. In this notebook, the selected model was the ResNet152V2 available on the Keras Package [(link)](https://keras.io/api/applications/resnet/#resnet152v2-function). \n",
    "\n",
    "This model was already trained in another dataset (ImageNet). What we do here is to set include_top to false, removing the â€˜headâ€™, responsible for assigning the classes in this other dataset, and keep all the previous layers. Then, we include our last few layers, including the one responsible for generating the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRw0D1FjyaOF"
   },
   "outputs": [],
   "source": [
    "base_model_xception = tf.keras.applications.Xception(\n",
    "    weights='imagenet',\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False, \n",
    "     classes = 2)\n",
    "\n",
    "base_model_resnet = tf.keras.applications.ResNet152V2(\n",
    "    weights='imagenet',\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False)\n",
    "\n",
    "base_model_efficientnet = tf.keras.applications.EfficientNetV2S(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(IMG_SIZE_EFF, IMG_SIZE_EFF, 3),\n",
    "    classes=2,\n",
    "    classifier_activation=\"softmax\"\n",
    ")\n",
    "\n",
    "base_model_xception.trainable = False\n",
    "base_model_resnet.trainable = False\n",
    "base_model_efficientnet.trainable = False\n",
    "\n",
    "def get_pretrained(architecture):\n",
    "    \n",
    "    #Input shape = [width, height, color channels]\n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    if architecture.lower() == \"xception\":\n",
    "        x = base_model_xception(inputs)\n",
    "    elif architecture.lower() == \"efficientnet\":\n",
    "        inputs = layers.Input(shape=(IMG_SIZE_EFF, IMG_SIZE_EFF, 3))\n",
    "        x = base_model_efficientnet(inputs)\n",
    "    elif architecture.lower() == \"resnet\":\n",
    "        x = base_model_resnet(inputs)\n",
    "\n",
    "    # Head\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    #Final Layer (Output)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=[inputs], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrtmIDSCth7l"
   },
   "outputs": [],
   "source": [
    "#Setting callbakcs\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=1e-7,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "plateau = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor = 0.2,                                     \n",
    "    patience = 4,                                   \n",
    "    min_delt = 1e-7,                                \n",
    "    cooldown = 0,                               \n",
    "    verbose = 1\n",
    ") \n",
    "\n",
    "# define the ModelCheckpoint callback\n",
    "\n",
    "checkpoint_path_efficientnet = \"model_checkpoint_efficientnet.h5\"\n",
    "checkpoint_callback_efficientnet = ModelCheckpoint(filepath=checkpoint_path_efficientnet,\n",
    "                                      save_weights_only=True,\n",
    "                                      monitor='val_loss',\n",
    "                                      save_best_only=True)\n",
    "\n",
    "checkpoint_path_resnet = \"model_checkpoint_resnet.h5\"\n",
    "checkpoint_callback_resnet = ModelCheckpoint(filepath=checkpoint_path_resnet,\n",
    "                                      save_weights_only=True,\n",
    "                                      monitor='val_loss',\n",
    "                                      save_best_only=True)\n",
    "\n",
    "checkpoint_path_xception = \"model_checkpoint_xception.h5\"\n",
    "checkpoint_callback_xception = ModelCheckpoint(filepath=checkpoint_path_xception,\n",
    "                                      save_weights_only=True,\n",
    "                                      monitor='val_loss',\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now print the different models we are going to be comparing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8GNTZfDyaOG",
    "outputId": "496a3859-e5cc-4c3a-971b-0b01bf3fa520"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model_pretrained_xception = get_pretrained(\"xception\")\n",
    "model_pretrained_xception.compile(loss='binary_crossentropy'\n",
    "              , optimizer = keras.optimizers.Adam(learning_rate=5e-4), metrics='binary_accuracy')\n",
    "\n",
    "model_pretrained_xception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model_pretrained_resnet = get_pretrained(\"resnet\")\n",
    "model_pretrained_resnet.compile(loss='binary_crossentropy'\n",
    "              , optimizer = keras.optimizers.Adam(learning_rate=5e-4), metrics='binary_accuracy')\n",
    "\n",
    "model_pretrained_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model_pretrained_efficientnet = get_pretrained(\"efficientnet\")\n",
    "model_pretrained_efficientnet.compile(loss='binary_crossentropy'\n",
    "              , optimizer = keras.optimizers.Adam(learning_rate=5e-4), metrics='binary_accuracy')\n",
    "\n",
    "model_pretrained_efficientnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "id": "fpoFp7ljyaOG"
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    history_xception = model_pretrained_xception.fit(ds_train,\n",
    "          batch_size = BATCH, epochs = epos,\n",
    "          validation_data=ds_val,\n",
    "          callbacks=[early_stopping, plateau, checkpoint_callback_xception],\n",
    "          steps_per_epoch=(len(train_df)/BATCH),\n",
    "          validation_steps=(len(val_df)/BATCH),\n",
    "          class_weight=class_weights);\n",
    "    \n",
    "    history_resnet = model_pretrained_resnet.fit(ds_train,\n",
    "          batch_size = BATCH, epochs = epos,\n",
    "          validation_data=ds_val,\n",
    "          callbacks=[early_stopping, plateau, checkpoint_callback_resnet],\n",
    "          steps_per_epoch=(len(train_df)/BATCH),\n",
    "          validation_steps=(len(val_df)/BATCH),\n",
    "          class_weight=class_weights);\n",
    "    \n",
    "    history_efficientnet = model_pretrained_efficientnet.fit(ds_train_efficient,\n",
    "          batch_size = BATCH, epochs = epos,\n",
    "          validation_data=ds_val_efficient,\n",
    "          callbacks=[early_stopping, plateau, checkpoint_callback_efficientnet],\n",
    "          steps_per_epoch=(len(train_df)/BATCH),\n",
    "          validation_steps=(len(val_df)/BATCH),\n",
    "          class_weight=class_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "\n",
    "sns.lineplot(x = history_resnet.epoch, y = history_resnet.history['loss'], label = \"Resnet Training loss\", marker = \"o\", color = \"blue\", markersize = 10)\n",
    "sns.lineplot(x = history_resnet.epoch, y = history_resnet.history['val_loss'], label = \"Resnet Validation Loss\", marker = \"o\", color = \"red\", markersize = 10)\n",
    "\n",
    "sns.lineplot(x = history_xception.epoch, y = history_xception.history['loss'], label = \"Xception Training loss\", marker = \"*\", color = \"blue\", markersize = 10)\n",
    "sns.lineplot(x = history_xception.epoch, y = history_xception.history['val_loss'], label = \"Xception Validation Loss\", marker = \"*\", color = \"red\", markersize = 10)\n",
    "\n",
    "sns.lineplot(x = history_efficientnet.epoch, y = history_efficientnet.history['loss'], label = \"Efficientnet Training loss\", marker = \"X\", color = \"blue\", markersize = 10)\n",
    "sns.lineplot(x = history_efficientnet.epoch, y = history_efficientnet.history['val_loss'], label = \"Efficientnet Validation Loss\", marker = \"X\", color = \"red\", markersize = 10)\n",
    "\n",
    "ax.set_title('Learning Curve (Loss)')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylim(0.05, 0.9)\n",
    "ax.legend()\n",
    "plt.savefig(\"Loss_curve_before_fine-tuning.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "qdDvq6e-yaOG"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "\n",
    "\n",
    "sns.lineplot(x = history_resnet.epoch, y = history_resnet.history['binary_accuracy'], label = \"Resnet Binary Accuracy\", marker = \"o\", color = \"blue\", markersize = 10)\n",
    "sns.lineplot(x = history_resnet.epoch, y = history_resnet.history['val_binary_accuracy'], label = \"Resnet Validation Binary Accuracy\", marker = \"o\", color = \"red\", markersize = 10)\n",
    "\n",
    "sns.lineplot(x = history_xception.epoch, y = history_xception.history['binary_accuracy'], label = \"Xception Training Binary Accuracy\", marker = \"*\", color = \"blue\", markersize = 10)\n",
    "sns.lineplot(x = history_xception.epoch, y = history_xception.history['val_binary_accuracy'], label = \"Xception Validation Binary Accuracy\", marker = \"*\", color = \"red\", markersize = 10)\n",
    "\n",
    "sns.lineplot(x = history_efficientnet.epoch, y = history_efficientnet.history['binary_accuracy'], label = \"Efficientnet Training Binary Accuracy\", marker = \"X\", color = \"blue\", markersize = 10)\n",
    "sns.lineplot(x = history_efficientnet.epoch, y = history_efficientnet.history['val_binary_accuracy'], label = \"Efficientnet Validation Binary Accuracy\", marker = \"X\", color = \"red\", markersize = 10)\n",
    "\n",
    "\n",
    "ax.set_title('Learning Curve (Accuracy)')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylim(0.7, 1.0)\n",
    "ax.legend()\n",
    "plt.savefig(\"Accuracy_curve_before_fine-tuning.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we print the acccuracy of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNXnrwtVyaOG"
   },
   "outputs": [],
   "source": [
    "score = model_pretrained_xception.evaluate(ds_val, steps = len(val_df)/BATCH, verbose = 0)\n",
    "print(\"Scores for the Xception model:\")\n",
    "print('Val loss:', score[0])\n",
    "print('Val accuracy:', score[1])\n",
    "val_accuracies.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_pretrained_resnet.evaluate(ds_val, steps = len(val_df)/BATCH, verbose = 0)\n",
    "print(\"Scores for the resnet model:\")\n",
    "print('Val loss:', score[0])\n",
    "print('Val accuracy:', score[1])\n",
    "val_accuracies.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_pretrained_efficientnet.evaluate(ds_val_efficient, steps = len(val_df)/BATCH, verbose = 0)\n",
    "print(\"Scores for the efficientnet model:\")\n",
    "print('Val loss:', score[0])\n",
    "print('Val accuracy:', score[1])\n",
    "val_accuracies.append(score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we now evalute all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "id": "l76AonvvyaOH"
   },
   "outputs": [],
   "source": [
    "test_accuracies = []\n",
    "score = model_pretrained_xception.evaluate(ds_test, steps = len(df_test), verbose = 0)\n",
    "print(\"Evaluation for the for the xception model:\")\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "test_accuracies.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_pretrained_resnet.evaluate(ds_test, steps = len(df_test), verbose = 0)\n",
    "print(\"Evaluation for the for the resnet model:\")\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "test_accuracies.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_pretrained_efficientnet.evaluate(ds_test_efficient, steps = len(df_test), verbose = 0)\n",
    "print(\"Evaluation for the for the efficientnet model:\")\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "test_accuracies.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_fine_tuning = {\"Model\":[\"Xception\", \"Resnet\", \"Efficientnet\"], \"Validation Accuracy\":val_accuracies, \"Test Accuracy\":test_accuracies}\n",
    "before_fine_tuning = pd.DataFrame(before_fine_tuning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Validation Accuracy',\n",
    "    x=['Xception', 'Resnet', 'Efficientnet'], y=val_accuracies, text = val_accuracies))\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Test Accuracy',\n",
    "    x=['Xception', 'Resnet', 'Efficientnet'], y=test_accuracies, text=test_accuracies\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=\"Accuracies before fine tuning the models\",\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Arbitrary units\",\n",
    "    legend_title=\"Dataset\",\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"Accuracies_before_fine-tuning.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkKo2_HcyaOH"
   },
   "source": [
    "# <a id=\"7\">Fine Tuning</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfSmIgYCyaOH"
   },
   "source": [
    "Our last approach is called Fine Tuning. In the last section, all the layers from the pretrained model were â€˜frozenâ€™, preserving the weights calculated during its training on the ImageNet dataset. Now, we are going to unfreeze a few of its last layers and continue the training, tuning the weights from these layers according to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bt3VNvAIyaOH"
   },
   "outputs": [],
   "source": [
    "base_model_xception.trainable = True\n",
    "base_model_resnet.trainable = True\n",
    "base_model_efficientnet.trainable = True\n",
    "\n",
    "# Unfreeze all layers except for the last 13\n",
    "#for layer in base_model.layers[:-13]:\n",
    "#    layer.trainable = False\n",
    "\n",
    "unfreeze_xception = 2 #Highest numbers\n",
    "unfreeze_resnet = 3\n",
    "unfreeze_efficientnet = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We unfreeze the models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count = 0    \n",
    "for layer in base_model_xception.layers[::-1]: #We reverse the array\n",
    "    if (\"conv\" in layer.name) and (count < unfreeze_xception):\n",
    "        count += 1\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unfreezing for xception layers\n",
    "\"\"\"\n",
    "\n",
    "for layer in base_model_xception.layers[:-unfreeze_xception]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count = 0    \n",
    "for layer in base_model_resnet.layers[::-1]: #We reverse the array\n",
    "    if (\"conv\" in layer.name) and (count < unfreeze_resnet):\n",
    "        count += 1\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unfreezing for resnet layers\n",
    "\"\"\"\n",
    "\n",
    "for layer in base_model_resnet.layers[:-unfreeze_resnet]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count = 0    \n",
    "for layer in base_model_efficientnet.layers[::-1]: #We reverse the array\n",
    "    if (\"conv\" in layer.name) and (count < unfreeze_efficientnet):\n",
    "        count += 1\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unfreezing for efficientnet layers\n",
    "\"\"\"\n",
    "\n",
    "for layer in base_model_efficientnet.layers[:-unfreeze_efficientnet]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now we we see the summary of these new models to be trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJpbxPhjyaOI"
   },
   "outputs": [],
   "source": [
    "model_pretrained_xception = get_pretrained(\"xception\")\n",
    "model_pretrained_xception.compile(loss='binary_crossentropy'\n",
    "              , optimizer = keras.optimizers.Adam(learning_rate=2e-5), metrics='binary_accuracy')\n",
    "\n",
    "print(\"Summary of Xception model:\")\n",
    "model_pretrained_xception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained_resnet = get_pretrained(\"resnet\")\n",
    "model_pretrained_resnet.compile(loss='binary_crossentropy'\n",
    "              , optimizer = keras.optimizers.Adam(learning_rate=2e-5), metrics='binary_accuracy')\n",
    "\n",
    "print(\"Summary of Resnet model:\")\n",
    "model_pretrained_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained_efficientnet = get_pretrained(\"efficientnet\")\n",
    "model_pretrained_efficientnet.compile(loss='binary_crossentropy'\n",
    "              , optimizer = keras.optimizers.Adam(learning_rate=2e-5), metrics='binary_accuracy')\n",
    "\n",
    "print(\"Summary of Efficientnet model:\")\n",
    "model_pretrained_efficientnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "id": "DPLYgugoyaOI"
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    history_xception = model_pretrained_xception.fit(ds_train,\n",
    "          batch_size = BATCH, epochs = epos,\n",
    "          validation_data=ds_val,\n",
    "          callbacks=[early_stopping, plateau, checkpoint_callback_xception],\n",
    "          steps_per_epoch=(len(train_df)/BATCH),\n",
    "          validation_steps=(len(val_df)/BATCH),\n",
    "          class_weight=class_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    history_resnet = model_pretrained_resnet.fit(ds_train,\n",
    "          batch_size = BATCH, epochs = epos,\n",
    "          validation_data=ds_val,\n",
    "          callbacks=[early_stopping, plateau, checkpoint_callback_resnet],\n",
    "          steps_per_epoch=(len(train_df)/BATCH),\n",
    "          validation_steps=(len(val_df)/BATCH),\n",
    "          class_weight=class_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):    \n",
    "    history_efficientnet = model_pretrained_efficientnet.fit(ds_train_efficient,\n",
    "          batch_size = BATCH, epochs = epos,\n",
    "          validation_data=ds_val_efficient,\n",
    "          callbacks=[early_stopping, plateau, checkpoint_callback_efficientnet],\n",
    "          steps_per_epoch=(len(train_df)/BATCH),\n",
    "          validation_steps=(len(val_df)/BATCH),\n",
    "          class_weight=class_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "\n",
    "sns.lineplot(x = history_resnet.epoch, y = history_resnet.history['loss'], label = \"Resnet Training loss\", marker = \"o\", color = \"blue\", markersize = 10)\n",
    "sns.lineplot(x = history_resnet.epoch, y = history_resnet.history['val_loss'], label = \"Resnet Validation Loss\", marker = \"o\", color = \"red\", markersize = 10)\n",
    "\n",
    "sns.lineplot(x = history_xception.epoch, y = history_xception.history['loss'], label = \"Xception Training loss\", marker = \"*\", color = \"blue\", markersize = 10)\n",
    "sns.lineplot(x = history_xception.epoch, y = history_xception.history['val_loss'], label = \"Xception Validation Loss\", marker = \"*\", color = \"red\", markersize = 10)\n",
    "\n",
    "sns.lineplot(x = history_efficientnet.epoch, y = history_efficientnet.history['loss'], label = \"Efficientnet Training loss\", marker = \"X\", color = \"blue\", markersize = 10)\n",
    "sns.lineplot(x = history_efficientnet.epoch, y = history_efficientnet.history['val_loss'], label = \"Efficientnet Validation Loss\", marker = \"X\", color = \"red\", markersize = 10)\n",
    "\n",
    "ax.set_title('Learning Curve (Loss)')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylim(0.05, 0.9)\n",
    "ax.legend()\n",
    "plt.savefig(\"Loss_curve_after_fine-tuning.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "qdDvq6e-yaOG"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "\n",
    "\n",
    "sns.lineplot(x = history_resnet.epoch, y = history_resnet.history['binary_accuracy'], label = \"Resnet Binary Accuracy\", marker = \"o\", color = \"blue\", markersize = 10)\n",
    "sns.lineplot(x = history_resnet.epoch, y = history_resnet.history['val_binary_accuracy'], label = \"Resnet Validation Binary Accuracy\", marker = \"o\", color = \"red\", markersize = 10)\n",
    "\n",
    "sns.lineplot(x = history_xception.epoch, y = history_xception.history['binary_accuracy'], label = \"Xception Training Binary Accuracy\", marker = \"*\", color = \"blue\", markersize = 10)\n",
    "sns.lineplot(x = history_xception.epoch, y = history_xception.history['val_binary_accuracy'], label = \"Xception Validation Binary Accuracy\", marker = \"*\", color = \"red\", markersize = 10)\n",
    "\n",
    "sns.lineplot(x = history_efficientnet.epoch, y = history_efficientnet.history['binary_accuracy'], label = \"Efficientnet Training Binary Accuracy\", marker = \"X\", color = \"blue\", markersize = 10)\n",
    "sns.lineplot(x = history_efficientnet.epoch, y = history_efficientnet.history['val_binary_accuracy'], label = \"Efficientnet Validation Binary Accuracy\", marker = \"X\", color = \"red\", markersize = 10)\n",
    "\n",
    "\n",
    "ax.set_title('Learning Curve (Accuracy)')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylim(0.7, 1.0)\n",
    "ax.legend()\n",
    "plt.savefig(\"Accuracy_curve_after_fine-tuning.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we print the acccuracy of the models again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracies_after = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNXnrwtVyaOG"
   },
   "outputs": [],
   "source": [
    "score_xception = model_pretrained_xception.evaluate(ds_val, steps = len(val_df)/BATCH, verbose = 0)\n",
    "print(\"Scores for the Xception model:\")\n",
    "print('Val loss:', score_xception[0])\n",
    "print('Val accuracy:', score_xception[1])\n",
    "val_accuracies_after.append(score_xception[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_resnet = model_pretrained_resnet.evaluate(ds_val, steps = len(val_df)/BATCH, verbose = 0)\n",
    "print(\"Scores for the resnet model:\")\n",
    "print('Val loss:', score_resnet[0])\n",
    "print('Val accuracy:', score_resnet[1])\n",
    "val_accuracies_after.append(score_resnet[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_efficientnet = model_pretrained_efficientnet.evaluate(ds_val_efficient, steps = len(val_df)/BATCH, verbose = 0)\n",
    "print(\"Scores for the efficientnet model:\")\n",
    "print('Val loss:', score_efficientnet[0])\n",
    "print('Val accuracy:', score_efficientnet[1])\n",
    "val_accuracies_after.append(score_efficientnet[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we now evalute all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "id": "l76AonvvyaOH"
   },
   "outputs": [],
   "source": [
    "test_accuracies_after = []\n",
    "score_xception = model_pretrained_xception.evaluate(ds_test, steps = len(df_test), verbose = 0)\n",
    "print(\"Evaluation for the for the xception model:\")\n",
    "print('Test loss:', score_xception[0])\n",
    "print('Test accuracy:', score_xception[1])\n",
    "test_accuracies_after.append(score_xception[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_resnet = model_pretrained_resnet.evaluate(ds_test, steps = len(df_test), verbose = 0)\n",
    "print(\"Evaluation for the for the resnet model:\")\n",
    "print('Test loss:', score_resnet[0])\n",
    "print('Test accuracy:', score_resnet[1])\n",
    "test_accuracies_after.append(score_resnet[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_efficientnet = model_pretrained_efficientnet.evaluate(ds_test_efficient, steps = len(df_test), verbose = 0)\n",
    "print(\"Evaluation for the for the efficientnet model:\")\n",
    "print('Test loss:', score_efficientnet[0])\n",
    "print('Test accuracy:', score_efficientnet[1])\n",
    "test_accuracies_after.append(score_efficientnet[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_fine_tuning = {\"Model\":[\"Xception\", \"Resnet\", \"Efficientnet\"], \"Validation Accuracy\":val_accuracies_after,\n",
    "                      \"Test Accuracy\":test_accuracies_after, \n",
    "                     \"Unfrozen layers\":[unfreeze_xception, unfreeze_resnet, unfreeze_efficientnet]}\n",
    "after_fine_tuning = pd.DataFrame(after_fine_tuning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Validation Accuracy',\n",
    "    x=['Xception', 'Resnet', 'Efficientnet'], y=val_accuracies_after, text = val_accuracies_after))\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Test Accuracy',\n",
    "    x=['Xception', 'Resnet', 'Efficientnet'], y=test_accuracies_after, text = test_accuracies_after\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=\"Accuracies After fine tuning the models\",\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Arbitrary units\",\n",
    "    legend_title=\"Dataset\",\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"Accuracies_after_fine_tuning.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBegI-skyaOJ"
   },
   "source": [
    "As expected, the fine-tuning approach has reached the best score. We end this notebook by showing a few performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gECRIiWRyaOJ"
   },
   "source": [
    "# <a id=\"8\">Performance Metrics</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now test all of our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "k3wnRB-qyaOJ"
   },
   "outputs": [],
   "source": [
    "num_label = {'Normal': 0, 'Pneumonia' : 1}\n",
    "Y_test = df_test['class'].copy().map(num_label).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-WQUx4ByaOK"
   },
   "outputs": [],
   "source": [
    "ds_test.reset()\n",
    "ds_test_efficient.reset()\n",
    "\n",
    "predictions_xception = model_pretrained_xception.predict(ds_test, steps=len(ds_test), verbose=0)\n",
    "predictions_resnet = model_pretrained_resnet.predict(ds_test, steps=len(ds_test), verbose=0)\n",
    "predictions_efficientnet = model_pretrained_efficientnet.predict(ds_test_efficient, steps=len(ds_test), verbose=0)\n",
    "\n",
    "\n",
    "pred_labels_xception = np.where(predictions_xception >0.5, 1, 0)\n",
    "pred_labels_resnet = np.where(predictions_resnet >0.5, 1, 0)\n",
    "pred_labels_efficientnet = np.where(predictions_efficientnet >0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsVEiP6jyaOK"
   },
   "outputs": [],
   "source": [
    "print(\"Xception Accuracy: \", accuracy_score(Y_test, pred_labels_xception))\n",
    "print(\"Resnet Accuracy: \", accuracy_score(Y_test, pred_labels_resnet))\n",
    "print(\"Efficientnet Accuracy: \", accuracy_score(Y_test, pred_labels_efficientnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "id": "TyoXXAsZyaOK"
   },
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(Y_test, pred_labels_xception)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n",
    "\n",
    "plt.title(\"Confusion Matrix for Xception\")\n",
    "plt.xlabel(\"Predicted Label\", fontsize= 12)\n",
    "plt.ylabel(\"True Label\", fontsize= 12)\n",
    "\n",
    "plt.savefig(\"Xception_confusion_matrix.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(Y_test, pred_labels_resnet)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n",
    "\n",
    "plt.title(\"Confusion Matrix for Resnet\")\n",
    "plt.xlabel(\"Predicted Label\", fontsize= 12)\n",
    "plt.ylabel(\"True Label\", fontsize= 12)\n",
    "\n",
    "plt.savefig(\"Resnet_confusion_matrix.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(Y_test, pred_labels_efficientnet)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n",
    "\n",
    "plt.title(\"Confusion Matrix for Efficientnet\")\n",
    "plt.xlabel(\"Predicted Label\", fontsize= 12)\n",
    "plt.ylabel(\"True Label\", fontsize= 12)\n",
    "\n",
    "plt.savefig(\"Efficientnet_confusion_matrix.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics classification report for all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "fcejPGR0yaOK"
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_test, pred_labels_xception, labels = [0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_test, pred_labels_resnet, labels = [0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(Y_test, pred_labels_efficientnet, labels = [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally, we plot the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "ySt4IgS0yaOK"
   },
   "outputs": [],
   "source": [
    "roc_auc_xception = metrics.roc_auc_score(Y_test, predictions_xception)\n",
    "roc_auc_resnet = metrics.roc_auc_score(Y_test, predictions_resnet)\n",
    "roc_auc_efficientnet = metrics.roc_auc_score(Y_test, predictions_efficientnet)\n",
    "\n",
    "print('ROC_AUC Xception: ', roc_auc_xception)\n",
    "print('ROC_AUC Resnet: ', roc_auc_resnet)\n",
    "print('ROC_AUC Efficientnet: ', roc_auc_efficientnet)\n",
    "\n",
    "fpr_xception, tpr_xception, thresholds_xception = metrics.roc_curve(Y_test, predictions_xception)\n",
    "fpr_resnet, tpr_resnet, thresholds_resnet = metrics.roc_curve(Y_test, predictions_resnet)\n",
    "fpr_efficientnet, tpr_efficientnet, thresholds_efficientnet = metrics.roc_curve(Y_test, predictions_efficientnet)\n",
    "\n",
    "plt.plot(fpr_xception, tpr_xception, label = 'Xception ROC_AUC = %0.3f' % roc_auc_xception)\n",
    "plt.plot(fpr_resnet, tpr_resnet, label = 'Resnet ROC_AUC = %0.3f' % roc_auc_resnet)\n",
    "plt.plot(fpr_efficientnet, tpr_efficientnet, label = 'Efficientnet ROC_AUC = %0.3f' % roc_auc_efficientnet)\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\", fontsize= 12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize= 12)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"ROC Curve after training additional layers\")\n",
    "\n",
    "plt.savefig(\"ROC_Curve_after_training_additional_layers.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6eSqivMyaOL"
   },
   "source": [
    "The recall was close to 100%. Even without expertise on the medical field, itâ€™s reasonable to assume that false negatives are more â€˜costlyâ€™ than false positives in this case. Reaching such recall with a relatively small dataset for training as this one, while also reaching a pretty good recall, is a good indicative of the modelâ€™s capabilities. Such capabilities are also confirmed by the high ROC-AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total elapsed time:\", time.time()- initial_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_fine_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Validation Accuracy Before Fine Tuning', marker = {\"color\":\"blue\"},\n",
    "    x=['Xception', 'Resnet', 'Efficientnet'], y=val_accuracies, text = val_accuracies))\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Validation Accuracy After Fine Tuning',marker = {\"color\":\"lightslategrey\"},\n",
    "    x=['Xception', 'Resnet', 'Efficientnet'], y=val_accuracies_after, text = val_accuracies_after\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=\"Validation accuracies comparison after and before fine tuning the models\",\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Arbitrary units\",\n",
    "    legend_title=\"Dataset\",\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"Accuracies_before_and_after_fine_tuning.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Validation Accuracy Before Fine Tuning', marker = {\"color\":\"red\"},\n",
    "    x=['Xception', 'Resnet', 'Efficientnet'], y=test_accuracies, text = test_accuracies))\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Test Accuracy After Fine Tuning',marker = {\"color\":\"orange\"},\n",
    "    x=['Xception', 'Resnet', 'Efficientnet'], y=test_accuracies_after, text = test_accuracies_after\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=\"Test accuracies comparison after and before fine tuning the models\",\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Arbitrary units\",\n",
    "    legend_title=\"Dataset\",\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"Test_Accuracies_before_and_after_fine_tuning.pdf\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
